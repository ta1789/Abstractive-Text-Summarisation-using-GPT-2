{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf11561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "#tqdm.notebook.trange\n",
    "import numpy as np\n",
    "#import pytorch_transformers\n",
    "from transformers import GPT2LMHeadModel,AdamW, get_linear_schedule_with_warmup\n",
    "#from pytorch_pretrained_bert.optimization import WarmupLinearSchedule\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import tqdm #import tnrange, tqdm_notebook\n",
    "\n",
    "from dataset import GPT21024Dataset \n",
    "from utils import add_special_tokens, generate_sample, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60717781",
   "metadata": {},
   "source": [
    "# Setting the necessary arguments required for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac462adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=5e-05, seed=42, n_gpu=4, gradient_accumulation_steps=2, batch_size=1, num_workers=4, device=device(type='cpu'), num_train_epochs=1, output_dir='C:/Users/ASUS/Downloads/output', model_dir='C:/Users/ASUS/Downloads/weights', max_grad_norm=1.0, root_dir='C:/Users/ASUS/Downloads/DN/gpt2_1024_data', ids_file='')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\",default=5e-5, type=float, help=\"learning rate\")\n",
    "parser.add_argument(\"--seed\",default=42, type=int,  help=\"seed to replicate results\")\n",
    "parser.add_argument(\"--n_gpu\",default=4, type=int,  help=\"no of gpu available\")\n",
    "parser.add_argument(\"--gradient_accumulation_steps\",default=2, type=int, help=\"gradient_accumulation_steps\")\n",
    "parser.add_argument(\"--batch_size\",default=1, type=int,  help=\"batch_size\")\n",
    "parser.add_argument(\"--num_workers\",default=4, type=int,  help=\"num of cpus available\")\n",
    "parser.add_argument(\"--device\",default=torch.device('cpu'), help=\"torch.device object\")\n",
    "parser.add_argument(\"--num_train_epochs\",default=1, type=int,  help=\"no of epochs of training\")\n",
    "parser.add_argument(\"--output_dir\",default='C:/Users/ASUS/Downloads/output', type=str,  help=\"path to save evaluation results\")\n",
    "parser.add_argument(\"--model_dir\",default='C:/Users/ASUS/Downloads/weights', type=str,  help=\"path to save trained model\")\n",
    "parser.add_argument(\"--max_grad_norm\",default=1.0, type=float, help=\"max gradient norm.\")\n",
    "parser.add_argument(\"--root_dir\",default='C:/Users/ASUS/Downloads/DN/gpt2_1024_data', type=str, help=\"location of json dataset.\")\n",
    "parser.add_argument(\"--ids_file\",default='', type=str, help=\"location of train, valid and test file indexes\")\n",
    "args = parser.parse_args([])\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57cdd35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, tokenizer, train_dataset, valid_dataset, ignore_index):\n",
    "    \"\"\" Trains GPT2 model and logs necessary details.\n",
    "        Args:\n",
    "            args: dict that contains all the necessary information passed by user while training\n",
    "            model: finetuned gpt/gpt2 model\n",
    "            tokenizer: GPT/GPT2 tokenizer\n",
    "            train_dataset: GPT21024Dataset object for training data\n",
    "            ignore_index: token not considered in loss calculation\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter('C:/Users/ASUS/Downloads/output/logs')\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    print(1)\n",
    "    train_dl = DataLoader(train_dataset,sampler=train_sampler,batch_size=args.batch_size,num_workers=args.num_workers)\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=args.lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,100,80000)\n",
    "    t=0\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = tqdm.notebook.trange(int(args.num_train_epochs), desc=\"Epoch\")\n",
    "    #tqdm.notebook.trange\n",
    "    set_seed(args)\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm.notebook.tqdm(train_dl, desc=\"Training\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            inputs, labels = batch['article'].to(args.device), batch['article'].to(args.device)\n",
    "            model.train()\n",
    "            logits = model(inputs)[0]\n",
    "            # only consider loss on reference summary just like seq2seq models\n",
    "            shift_logits = logits[..., batch['sum_idx']:-1, :].contiguous()\n",
    "            shift_labels = labels[..., batch['sum_idx']+1:].contiguous()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            loss = loss/args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "                writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                writer.add_scalar('loss', (tr_loss - logging_loss)/args.gradient_accumulation_steps, global_step)\n",
    "                logging_loss = tr_loss\n",
    "                print(\"loss:\", loss.item(), end='\\n\\n')\n",
    "                if (step + 1)/args.gradient_accumulation_steps == 1.0:\n",
    "                    print('After 1st update: ', end='\\n\\n')\n",
    "                    generate_sample(valid_dataset, tokenizer, model, num=1, eval_step=False,device=args.device)\n",
    "                \n",
    "                \n",
    "            if (step + 1) % (10*args.gradient_accumulation_steps) == 0:\n",
    "                results = evaluate(args, model, valid_dataset, ignore_index, global_step)\n",
    "                for key, value in results.items():\n",
    "                    writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "                print('After', global_step+1,'updates: ', end='\\n\\n')\n",
    "                generate_sample(valid_dataset, tokenizer, num=1, eval_step=True,device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5d0b886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(args.ids_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4371b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, eval_dataset, ignore_index, global_step=None):\n",
    "    \"\"\" Returns perplexity score on validation dataset.\n",
    "        Args:\n",
    "            args: dict that contains all the necessary information passed by user while training\n",
    "            model: finetuned gpt/gpt2 model\n",
    "            eval_dataset: GPT21024Dataset object for validation data\n",
    "            global_step: no. of times gradients have backpropagated\n",
    "            ignore_index: token not considered in loss calculation\n",
    "    \"\"\"\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.mkdir(args.output_dir)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    results = {}\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.batch_size)\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm.notebook.trange(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = batch['article'].to(args.device), batch['article'].to(args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)[0]\n",
    "            idx = batch['sum_idx'].item() # index of separator token\n",
    "            # only consider loss on reference summary just like seq2seq models\n",
    "            shift_logits = logits[..., batch['sum_idx']:-1, :].contiguous()\n",
    "            shift_labels = labels[..., batch['sum_idx']+1:].contiguous()\n",
    "            lm_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "    print(\"perplexity:\", perplexity.item())\n",
    "\n",
    "    if global_step:\n",
    "        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"a\") as f:\n",
    "            for key in sorted(result.keys()):\n",
    "                f.write('\\n\\n')\n",
    "                f.write(\"time = %s, %s = %s, step = %s\\n\" % (datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), key, str(result[key]), str(global_step)))\n",
    "    return result     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a021c1",
   "metadata": {},
   "source": [
    "# Creating training and validation dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54db68",
   "metadata": {},
   "source": [
    "train_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='train',length=10) \n",
    "valid_data = GPT21024Dataset(args.root_dir,args.ids_file,mode='valid',length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417c0c93",
   "metadata": {},
   "source": [
    "# Loading pretrained GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f080939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50259, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50259, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = add_special_tokens()\n",
    "ignore_idx = tokenizer.pad_token_id\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460263c",
   "metadata": {},
   "source": [
    "# training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "058a0356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2f33dc355f4301827c5a9c2bc80d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc83bdd6543d439da82e544dd0b6c40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 44.43930435180664\n",
      "\n",
      "After 1st update: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c865971896744324960724e978e79cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "nicolas dalby defends his welterweight title against mohsen bahari on saturday - and you can watch it live with mail online. as part of our exciting new partnership with cage warriors, we will bring you full coverage from london. watch all the action from 6pm.\n",
      "\n",
      "generated_summary\n",
      "\n",
      "\n",
      "\n",
      "actual_summary\n",
      "\n",
      "nicolas dalby takes on mohsen bahari at the copper box arena in london. dalby's welterweight title will be on the line at the olympic venue. pannie kianzad vs eeva siiskonen heads a packed undercard. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "loss: 45.80469512939453\n",
      "\n",
      "loss: 36.3017463684082\n",
      "\n",
      "loss: 41.84835433959961\n",
      "\n",
      "loss: 29.45965576171875\n",
      "\n",
      "total time:  84.5308799068133  minutes\n",
      "\n",
      "Saving trained model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start = time.time()\n",
    "train(args, model, tokenizer, train_data, valid_data, ignore_idx)\n",
    "print('total time: ', (time.time()-start)/60, \" minutes\", end='\\n\\n')\n",
    "\n",
    "print('Saving trained model...')\n",
    "model_file = os.path.join(args.model_dir, 'model_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.bin'.format(len(train_data),args.num_train_epochs))\n",
    "config_file = os.path.join(args.model_dir, 'config_data{}_trained_after_{}_epochs_only_sum_loss_ignr_pad.json'.format(len(train_data),args.num_train_epochs))\n",
    "torch.save(model.state_dict(), model_file)\n",
    "model.config.to_json_file(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e10c7",
   "metadata": {},
   "source": [
    "# Utility Function to Generate Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a0761d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tnrange\n",
    "\n",
    "\n",
    "def add_special_tokens():\n",
    "    \"\"\" Returns GPT2 tokenizer after adding separator and padding tokens \"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    special_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\n",
    "    num_add_toks = tokenizer.add_special_tokens(special_tokens)\n",
    "    return tokenizer\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_seq(model, context, length, device, temperature=1, top_k=0, top_p=0.0):\n",
    "    \"\"\" Generates a sequence of tokens \n",
    "        Args:\n",
    "            model: gpt/gpt2 model\n",
    "            context: tokenized text using gpt/gpt2 tokenizer\n",
    "            length: length of generated sequence.\n",
    "            device: torch.device object.\n",
    "            temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "    \"\"\"\n",
    "    \n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    generated = context\n",
    "    with torch.no_grad():  \n",
    "        for _ in tnrange(length):\n",
    "            inputs = {'input_ids': generated}\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n",
    "            next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "    return generated\n",
    "\n",
    "\n",
    "def beam_search(model, context, length, beam_size, device, temperature=1):\n",
    "    \"\"\" Generate sequence using beam search https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    "        Args:\n",
    "            model: gpt/gpt2 model\n",
    "            context: tokenized text using gpt/gpt2 tokenizer\n",
    "            length: length of generated sequence.\n",
    "            beam_size: >=1 and <= total_no_of_tokens\n",
    "            device: torch.device object.\n",
    "            temperature >0: used to control the randomness of predictions by scaling the logits before applying softmax.\n",
    "    \"\"\"\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0)\n",
    "    with torch.no_grad():  \n",
    "        inputs = {'input_ids': context}\n",
    "        outputs = model(**inputs) \n",
    "        next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "        next_token_probs = F.softmax(next_token_logits)\n",
    "        scores, indices = torch.topk(next_token_probs, beam_size)\n",
    "        indices = indices.tolist()\n",
    "        sequences = [[c] for c in indices]\n",
    "        for _ in tnrange(length-1):\n",
    "            logits = torch.zeros(beam_size*len(next_token_logits))\n",
    "            for j in range(len(sequences)):\n",
    "                new_generated = torch.cat((context,torch.tensor([sequences[j]], dtype=torch.long, device=device)),dim=1)\n",
    "                inputs = {'input_ids': new_generated}\n",
    "                outputs = model(**inputs) \n",
    "                next_token_logits = outputs[0][0, -1, :] / temperature\n",
    "                next_token_probs = F.softmax(next_token_logits)\n",
    "                start, stop = j*len(next_token_logits), (j+1)*len(next_token_logits)\n",
    "                logits[start:stop] = scores[j]*next_token_probs\n",
    "            scores, new_logits_indices = torch.topk(logits,beam_size)\n",
    "            logits = (new_logits_indices%50259).tolist()\n",
    "            for j in range(len(sequences)):\n",
    "                sequences[j] = sequences[j]+[logits[j]]\n",
    "    return scores, sequences\n",
    "\n",
    "\n",
    "def generate_beam_sample(data, tokenizer, model, num=1, length=100, beam_size=3, device=torch.device('cuda')):\n",
    "    \"\"\" Generate summaries for \"num\" number of articles using beam search.\n",
    "        Args:\n",
    "            data = GPT21024Dataset object\n",
    "            tokenizer = gpt/gpt2 tokenizer\n",
    "            num = number of articles for which summaries has to be generated\n",
    "    \"\"\"\n",
    "    for i in range(num):\n",
    "        sample = data[i]\n",
    "        idx = sample['sum_idx']\n",
    "        print(sample)\n",
    "        context = sample['article'][:idx].tolist()\n",
    "        summary = sample['article'][idx+1:][:100].tolist()\n",
    "        scores, sequences = beam_search(model, context, length, beam_size, device)\n",
    "        print('new_article', end='\\n\\n')\n",
    "        print(tokenizer.decode(context[:-1]), end='\\n\\n')\n",
    "        print('actual_summary', end='\\n\\n')\n",
    "        print(tokenizer.decode(summary), end='\\n\\n')\n",
    "        for i in range(len(sequences)):\n",
    "            text = tokenizer.convert_ids_to_tokens(sequences[i],skip_special_tokens=True)\n",
    "            text = tokenizer.convert_tokens_to_string(text)  \n",
    "            print(\"generated_summary-{} and Score is {}.\".format(i+1, scores[i]), end='\\n\\n')\n",
    "            print(text, end='\\n\\n')\n",
    "\n",
    "\n",
    "def generate_sample(data, tokenizer, model, num=1, eval_step=False, length=100, temperature=1, top_k=10, top_p=0.5, device=torch.device('cuda')):\n",
    "    \"\"\" Generate summaries for \"num\" number of articles.\n",
    "        Args:\n",
    "            data = GPT21024Dataset object\n",
    "            tokenizer = gpt/gpt2 tokenizer\n",
    "            model = gpt/gpt2 model\n",
    "            num = number of articles for which summaries has to be generated\n",
    "            eval_step = can be True/False, checks generating during evaluation or not\n",
    "    \"\"\"\n",
    "    for i in range(num):\n",
    "        sample = data[i]\n",
    "        idx = sample['sum_idx']\n",
    "        context = sample['article'][:idx].tolist()\n",
    "        summary = sample['article'][idx+1:][:100].tolist()\n",
    "        print(summary)\n",
    "        generated_text = sample_seq(model, context, length, device, temperature, top_k, top_p)\n",
    "        generated_text = generated_text[0, len(context):].tolist()\n",
    "        text = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n",
    "        text = tokenizer.convert_tokens_to_string(text)\n",
    "        s=tokenizer.decode(summary)\n",
    "        print(text)\n",
    "        if eval_step==False:\n",
    "            print('new_article', end='\\n\\n')\n",
    "            print(tokenizer.decode(context), end='\\n\\n')\n",
    "            print(\"generated_summary\", end='\\n\\n')\n",
    "            print(len(text), end='\\n\\n')\n",
    "            print('actual_summary', end='\\n\\n')\n",
    "            print(tokenizer.decode(summary), end='\\n\\n')\n",
    "            #rou=Rouge()\n",
    "            #print(\"Rouge Score:\",rou.get_scores(tokenizer.decode(summary),text))\n",
    "        else:\n",
    "            print(tokenizer.decode(context), end='\\n\\n')\n",
    "            print(\"generated_summary\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe17fe0",
   "metadata": {},
   "source": [
    "# Generating Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5225b1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6988, 12456, 288, 282, 1525, 2753, 319, 285, 1219, 6248, 275, 993, 2743, 379, 262, 15317, 3091, 13478, 287, 300, 3391, 764, 288, 282, 1525, 705, 82, 5029, 353, 6551, 3670, 481, 307, 319, 262, 1627, 379, 262, 267, 6760, 291, 14359, 764, 279, 42883, 479, 666, 89, 324, 3691, 304, 48855, 33721, 1984, 34481, 6665, 257, 11856, 739, 9517, 764, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20064\\1646315985.py:71: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(length):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce81be357a744358541d1121e8fa2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "new_article\n",
      "\n",
      "nicolas dalby defends his welterweight title against mohsen bahari on saturday - and you can watch it live with mail online. as part of our exciting new partnership with cage warriors, we will bring you full coverage from london. watch all the action from 6pm.\n",
      "\n",
      "generated_summary\n",
      "\n",
      "0\n",
      "\n",
      "actual_summary\n",
      "\n",
      "nicolas dalby takes on mohsen bahari at the copper box arena in london. dalby's welterweight title will be on the line at the olympic venue. pannie kianzad vs eeva siiskonen heads a packed undercard. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_sample(train_data, tokenizer, model, num=1,length=10,eval_step=False,device=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b795b847",
   "metadata": {},
   "source": [
    "# Generating Beam Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27e970b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': tensor([ 6988, 12456,   288,  ..., 50257, 50257, 50257]), 'sum_idx': 61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20064\\1646315985.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  next_token_probs = F.softmax(next_token_logits)\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20064\\1646315985.py:101: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for _ in tnrange(length-1):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ca6d489da44076a2524583410ff512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20064\\1646315985.py:108: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  next_token_probs = F.softmax(next_token_logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "nicolas dalby defends his welterweight title against mohsen bahari on saturday - and you can watch it live with mail online. as part of our exciting new partnership with cage warriors, we will bring you full coverage from london. watch all the action from 6pm\n",
      "\n",
      "actual_summary\n",
      "\n",
      "nicolas dalby takes on mohsen bahari at the copper box arena in london. dalby's welterweight title will be on the line at the olympic venue. pannie kianzad vs eeva siiskonen heads a packed undercard. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "generated_summary-1 and Score is 0.3455829620361328.\n",
      "\n",
      "\n",
      "\n",
      "generated_summary-2 and Score is 0.1763933151960373.\n",
      "\n",
      "\n",
      "\n",
      "generated_summary-3 and Score is 0.1584562510251999.\n",
      "\n",
      " .\n",
      "\n",
      "{'article': tensor([ 1525, 46078, 29955,  ..., 50257, 50257, 50257]), 'sum_idx': 44}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201a7fd3090b47a5b6f0be141d7ead19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_article\n",
      "\n",
      "by dan bloom. these are the 12 mummified corpses which were unearthed in a neighbourhood of mexico city - and promptly adopted as beloved citizens. the remains lay undiscovered beneath a monastery in san Ã£\n",
      "\n",
      "actual_summary\n",
      "\n",
      "12 corpses were discovered beneath monastery in san angel, mexico city. they had been sealed away when catholic college was abandoned in 1861. townsfolk urged to rebury mummies but insisted they were fellow citizens. so they were put in velvet-lined caskets where they remained for 85 years. <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\n",
      "\n",
      "generated_summary-1 and Score is 0.7597846984863281.\n",
      "\n",
      "\n",
      "\n",
      "generated_summary-2 and Score is 0.061154209077358246.\n",
      "\n",
      "\n",
      "\n",
      "generated_summary-3 and Score is 0.03527423366904259.\n",
      "\n",
      " the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_beam_sample(train_data, tokenizer, model, num=2, length=10, beam_size=3, device=args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a53eb46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
